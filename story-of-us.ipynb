{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "189a64c4",
   "metadata": {},
   "source": [
    "## Process\n",
    "1. Upload photos, get metadata if possible (location, date, etc.)\n",
    "2. Send descriptions through vision model to get descriptions, append to images in an object (i.e. {file: str, description: str})\n",
    "3. Prompt model with photo objects, output should be an object with file name, transcript text chunk and duration\n",
    "4. Run through opencv to create slideshow\n",
    "5. Create audio with TTS model\n",
    "6. combine audio and video with tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd07df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata for images\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS\n",
    "import os\n",
    "\n",
    "def get_basic_info(image_path):\n",
    "    \"\"\"Extract filename, datetime, and size in MB from image.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    exif = image._getexif() if hasattr(image, '_getexif') else None\n",
    "    \n",
    "    # Get file size in MB\n",
    "    size_mb = os.path.getsize(image_path) / (1024 * 1024)\n",
    "    \n",
    "    # Get datetime if available\n",
    "    datetime = None\n",
    "    if exif:\n",
    "        for tag_id in exif:\n",
    "            tag = TAGS.get(tag_id, tag_id)\n",
    "            if tag == 'DateTime':\n",
    "                datetime = exif[tag_id]\n",
    "    \n",
    "    return {\n",
    "        'filename': os.path.basename(image_path),\n",
    "        'datetime': datetime,\n",
    "        'size_mb': round(size_mb, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e511f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set openai api key\n",
    "os.environ['OPENAI_API_KEY'] = \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0432d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send through vision model to get descriptions\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "def get_photo_description(photo_path):\n",
    "    \"\"\"\n",
    "    Get a description of a photo using GPT-4o-mini\n",
    "    \n",
    "    Args:\n",
    "        photo_path (str): Path to the photo file\n",
    "        \n",
    "    Returns:\n",
    "        str: Description of the photo\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    \n",
    "    # Encode the image to base64\n",
    "    def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    base64_image = encode_image(photo_path)\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            # Tried having it output longer content and told it it was for a sentimenta/romantic movie but it just tried way too hard lol\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        # new prompt, haven't tried it yet but want to\n",
    "                        {\"type\": \"text\", \"text\": \"Please describe this photo in a concise way, add cute little details about what is happening but still keep it brief\"},\n",
    "                        # old prompt {\"type\": \"text\", \"text\": \"Please describe this photo in a concise way. The woman in the photo is my wife, Payton, the dog is Summit, and the man is me, Michael.\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting description for {photo_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Now let's modify the format_photos function to include descriptions\n",
    "def format_photos(photo_paths):\n",
    "    formatted_photos = []\n",
    "    \n",
    "    for path in photo_paths:\n",
    "        try:\n",
    "            basic_info = get_basic_info(path)\n",
    "            description = get_photo_description(path)\n",
    "            \n",
    "            photo_object = {\n",
    "                'image_path': path,\n",
    "                'date_time': basic_info['datetime'],\n",
    "                'size': basic_info['size_mb'],\n",
    "                'description': description\n",
    "            }\n",
    "            formatted_photos.append(photo_object)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {path}: {str(e)}\")\n",
    "            \n",
    "    return formatted_photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Assumes you have a /photos directory with photos to read\n",
    "photos_dir = 'photos'\n",
    "photos = [os.path.join(photos_dir, f) for f in os.listdir(photos_dir) \n",
    "          if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
    "\n",
    "formatted_photos = format_photos(photos)\n",
    "formatted_photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "59218355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transcript by using structured output of images and descriptions as segments\n",
    "#got-4o is much better at this than gpt-4o-mini at this, it includes more photos and the poem is better overall\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "class SlideSegment(BaseModel):\n",
    "    image_number: int\n",
    "    duration_seconds: float\n",
    "    text: str\n",
    "    image_path: str\n",
    "\n",
    "class SlideshowTranscript(BaseModel):\n",
    "    segments: List[SlideSegment]\n",
    "\n",
    "def generate_slideshow_transcript(formatted_photos):\n",
    "    \"\"\"\n",
    "    Generate a slideshow transcript from formatted photos using GPT-4\n",
    "    \n",
    "    Args:\n",
    "        formatted_photos (list): List of dictionaries containing photo information\n",
    "        \n",
    "    Returns:\n",
    "        List[SlideSegment]: List of structured segments for the slideshow\n",
    "    \"\"\"\n",
    "    # Sort photos by date for chronological ordering\n",
    "    sorted_photos = sorted(formatted_photos, key=lambda x: x['date_time'])\n",
    "    \n",
    "    # Create a focused context with paths and descriptions\n",
    "    photo_context = [\n",
    "        {\n",
    "            'path': photo['image_path'],\n",
    "            'date': photo['date_time'],\n",
    "            'description': photo['description']\n",
    "        } for photo in sorted_photos\n",
    "    ]\n",
    "    \n",
    "    # Extract just paths for validation\n",
    "    valid_paths = [p['path'] for p in photo_context]\n",
    "    client = OpenAI()\n",
    "    \n",
    "    \n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are a professional narrator creating a sentimental slideshow for my wife, Payton.\n",
    "                AVAILABLE IMAGES:\n",
    "                Below is a chronological list of photos with their descriptions. Use these descriptions to craft accurate, meaningful segments:\n",
    "\n",
    "                {json.dumps(photo_context, indent=2)}\n",
    "\n",
    "                CRITICAL RULES:\n",
    "                1. ONLY use image paths from this list - no exceptions: {valid_paths}\n",
    "                2. Base your narrative on the actual content described in each image\n",
    "                3. Follow the chronological order of the photos\n",
    "                4. Keep each segment's duration between 2-5 seconds\n",
    "                5. Reference specific details from the image descriptions in your narrative\n",
    "                6. Don't make assumptions about events not shown in the photos\n",
    "\n",
    "                Your task is to create a rhyming poem that:\n",
    "                - Starts with \"The story of us goes like this...\"\n",
    "                - Uses modern, relatable language (think Taylor Swift style)\n",
    "                - Makes natural transitions between images\n",
    "                - Incorporates specific details from the image descriptions\n",
    "                - Tells the story through what's actually visible in each photo per the descriptions\n",
    "\n",
    "                For each segment:\n",
    "                - Choose an exact image path from the list\n",
    "                - Set an appropriate duration (2-5 seconds)\n",
    "                - Write a rhyming line that matches the photo's description\n",
    "                - Focus on the actual elements described in the photo\n",
    "\n",
    "                Remember: If you need a specific type of image for your narrative but can't find one that matches in the descriptions, adjust your narrative to fit what's actually available. Never invent scenes or modify image paths.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Create a slideshow narrative that follows these photos chronologically, using their descriptions to tell an accurate and meaningful story. Every image_path must match exactly from the provided list.\"\n",
    "            }\n",
    "                ],\n",
    "        response_format=SlideshowTranscript\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.parsed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "67440b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_segments = generate_slideshow_transcript(formatted_photos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6f6650dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes, the transcript will reference a photo that doesn't exist, this function finds those photos so we can replace them or run the function again\n",
    "def find_missing_paths(formatted_photos, transcript_segments):\n",
    "    \"\"\"\n",
    "    Find paths in transcript that don't exist in formatted_photos\n",
    "    \n",
    "    Args:\n",
    "        formatted_photos (list): List of dictionaries containing photo information\n",
    "        transcript_segments (SlideshowTranscript): Transcript with timing and image information\n",
    "        \n",
    "    Returns:\n",
    "        set: Set of paths that exist in transcript but not in formatted_photos\n",
    "    \"\"\"\n",
    "    formatted_paths = set(photo['image_path'] for photo in formatted_photos)\n",
    "    transcript_paths = set(segment.image_path for segment in transcript_segments.segments)\n",
    "    return transcript_paths - formatted_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_paths = find_missing_paths(formatted_photos, transcript_segments)\n",
    "missing_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f842359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "for segment in transcript_segments.segments:\n",
    "    print(f\"Segment {segment.image_number}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Image Path: {segment.image_path}\")\n",
    "    print(f\"Duration: {segment.duration_seconds} seconds\")\n",
    "    print(f\"Text: {segment.text}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0f7d853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create slideshow with opencv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def resize_with_padding(image, target_width, target_height):\n",
    "    # Get current and target aspect ratios\n",
    "    target_aspect = target_width / target_height\n",
    "    image_aspect = image.shape[1] / image.shape[0]\n",
    "    \n",
    "    if image_aspect > target_aspect:\n",
    "        # Image is wider than target: fit to width\n",
    "        new_width = target_width\n",
    "        new_height = int(target_width / image_aspect)\n",
    "        resized = cv2.resize(image, (new_width, new_height))\n",
    "        \n",
    "        # Add black bars on top and bottom (letterbox)\n",
    "        top_padding = (target_height - new_height) // 2\n",
    "        bottom_padding = target_height - new_height - top_padding\n",
    "        padded = cv2.copyMakeBorder(\n",
    "            resized,\n",
    "            top_padding,\n",
    "            bottom_padding,\n",
    "            0,\n",
    "            0,\n",
    "            cv2.BORDER_CONSTANT,\n",
    "            value=[0, 0, 0]\n",
    "        )\n",
    "    else:\n",
    "        # Image is taller than target: fit to height\n",
    "        new_height = target_height\n",
    "        new_width = int(target_height * image_aspect)\n",
    "        resized = cv2.resize(image, (new_width, new_height))\n",
    "        \n",
    "        # Add black bars on left and right (pillarbox)\n",
    "        left_padding = (target_width - new_width) // 2\n",
    "        right_padding = target_width - new_width - left_padding\n",
    "        padded = cv2.copyMakeBorder(\n",
    "            resized,\n",
    "            0,\n",
    "            0,\n",
    "            left_padding,\n",
    "            right_padding,\n",
    "            cv2.BORDER_CONSTANT,\n",
    "            value=[0, 0, 0]\n",
    "        )\n",
    "    \n",
    "    return padded\n",
    "\n",
    "def create_slideshow(json_data, output_path='slideshow.mp4', fps=30, target_width=1920, target_height=1080):\n",
    "    start_time = time.time()\n",
    "    segments = json_data['segments']\n",
    "    \n",
    "    # Calculate total frames\n",
    "    total_frames = sum(int(segment['duration_seconds'] * fps) for segment in segments)\n",
    "    \n",
    "    # Initialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (target_width, target_height))\n",
    "    \n",
    "    # Process each segment with progress bar\n",
    "    with tqdm(total=total_frames, desc=\"Creating slideshow\") as pbar:\n",
    "        for segment in segments:\n",
    "            img = cv2.imread(segment['image_path'])\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image {segment['image_path']}\")\n",
    "                continue\n",
    "            \n",
    "            # Resize image and add black bars while maintaining aspect ratio\n",
    "            img = resize_with_padding(img, target_width, target_height)\n",
    "            \n",
    "            # Write the image for duration * fps frames\n",
    "            n_frames = int(segment['duration_seconds'] * fps)\n",
    "            for _ in range(n_frames):\n",
    "                out.write(img)\n",
    "                pbar.update(1)\n",
    "    \n",
    "    out.release()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nSlideshow created in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Output saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d470176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert Pydantic model to compatible dictionary format\n",
    "slideshow_dict = {\n",
    "    'segments': [\n",
    "        {\n",
    "            'image_path': segment.image_path,\n",
    "            'duration_seconds': segment.duration_seconds,\n",
    "            'text': segment.text\n",
    "        }\n",
    "        for segment in transcript_segments.segments\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Use existing create_slideshow function\n",
    "create_slideshow(slideshow_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4a050016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create audio with TTS model\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def create_tts_for_segments(transcript_segments, output_dir=\"tts_segments\"):\n",
    "    \"\"\"\n",
    "    Create TTS audio for each segment in the transcript, matching segment durations\n",
    "    \n",
    "    Args:\n",
    "        transcript_segments (SlideshowTranscript): Transcript with timing and text information\n",
    "        output_dir (str): Directory to save individual audio files\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the combined audio file\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    combined_audio = AudioSegment.silent(duration=0)\n",
    "    \n",
    "    for i, segment in enumerate(transcript_segments.segments):\n",
    "        speech_file_path = f\"{output_dir}/segment_{i}.mp3\"\n",
    "        \n",
    "        # Generate TTS audio\n",
    "        response = client.audio.speech.create(\n",
    "            model=\"tts-1\",\n",
    "            voice=\"alloy\",\n",
    "            input=segment.text\n",
    "        )\n",
    "        \n",
    "        # Save the audio file\n",
    "        response.stream_to_file(speech_file_path)\n",
    "        \n",
    "        # Load the audio segment\n",
    "        segment_audio = AudioSegment.from_mp3(speech_file_path)\n",
    "        \n",
    "        # Convert segment duration from seconds to milliseconds\n",
    "        target_duration_ms = int(segment.duration_seconds * 1000)\n",
    "        current_duration_ms = len(segment_audio)\n",
    "        \n",
    "        if current_duration_ms < target_duration_ms:\n",
    "            # If audio is shorter than segment, add silence at the end\n",
    "            silence_duration = target_duration_ms - current_duration_ms\n",
    "            segment_audio = segment_audio + AudioSegment.silent(duration=silence_duration)\n",
    "        elif current_duration_ms > target_duration_ms:\n",
    "            # If audio is longer than segment, log a warning\n",
    "            print(f\"Warning: Audio for segment {i} is longer than segment duration\")\n",
    "            print(f\"Audio: {current_duration_ms/1000:.2f}s, Segment: {segment.duration_seconds:.2f}s\")\n",
    "        \n",
    "        # Add to combined audio\n",
    "        combined_audio += segment_audio\n",
    "    \n",
    "    # Save the combined audio\n",
    "    combined_file_path = f\"{output_dir}/combined_narration.mp3\"\n",
    "    combined_audio.export(combined_file_path, format=\"mp3\")\n",
    "    \n",
    "    # Print total duration\n",
    "    print(f\"Total audio duration: {len(combined_audio)/1000:.2f} seconds\")\n",
    "    \n",
    "    return combined_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76faf7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "narration_path = create_tts_for_segments(transcript_segments)\n",
    "print(f\"Combined narration saved to: {narration_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "54026d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine video and audio with moviepy\n",
    "from moviepy import VideoFileClip, AudioFileClip, CompositeAudioClip\n",
    "\n",
    "def combine_video_and_narration(video_path, narration_path, output_path):\n",
    "    \"\"\"\n",
    "    Combine video with narration audio using CompositeAudioClip\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file\n",
    "        narration_path (str): Path to the narration audio file\n",
    "        output_path (str): Path for the output video\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load video and audio clips\n",
    "        videoclip = VideoFileClip(video_path)\n",
    "        audioclip = AudioFileClip(narration_path)\n",
    "\n",
    "        # Ensure narration duration matches video duration\n",
    "        if audioclip.duration > videoclip.duration:\n",
    "            print(f\"Warning: Narration ({audioclip.duration:.2f}s) is longer than video ({videoclip.duration:.2f}s)\")\n",
    "        elif audioclip.duration < videoclip.duration:\n",
    "            print(f\"Warning: Narration ({audioclip.duration:.2f}s) is shorter than video ({videoclip.duration:.2f}s)\")\n",
    "\n",
    "        # Create composite audio and set it to video\n",
    "        new_audioclip = CompositeAudioClip([audioclip])\n",
    "        videoclip.audio = new_audioclip\n",
    "\n",
    "         # Write the final video with higher quality settings\n",
    "        videoclip.write_videofile(output_path, \n",
    "                                   codec='libx264', \n",
    "                                   audio_codec='aac',\n",
    "                                   bitrate='8000k',  # Increased video bitrate\n",
    "                                   audio_bitrate='320k',  # Increased audio bitrate\n",
    "                                   preset='slow',  # Better compression, but slower encoding\n",
    "                                   threads=4)  # Use 4 threads for faster encoding\n",
    "        \n",
    "    finally:\n",
    "        # Clean up\n",
    "        videoclip.close()\n",
    "        audioclip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b6d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"slideshow.mp4\"\n",
    "narration_path = \"tts_segments/combined_narration.mp3\"\n",
    "output_path = \"final_slideshow_with_narration.mp4\"\n",
    "\n",
    "combine_video_and_narration(video_path, narration_path, output_path)\n",
    "print(f\"Final video saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edebd87",
   "metadata": {},
   "source": [
    "# Archived/Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b8e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads the transcript.json file and recreates the data models if u don't want to run the function again\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "# Recreate the data models\n",
    "class SlideSegment(BaseModel):\n",
    "    image_number: int\n",
    "    duration_seconds: float\n",
    "    text: str\n",
    "    image_path: str\n",
    "\n",
    "class SlideshowTranscript(BaseModel):\n",
    "    segments: List[SlideSegment]\n",
    "with open('transcript.json', 'r') as f:\n",
    "    transcript_dict = json.load(f)\n",
    "\n",
    "# Convert back to Pydantic model\n",
    "transcript_segments = SlideshowTranscript(**transcript_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee10793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved photos to formatted_photos_11_26_24_1.json\n"
     ]
    }
   ],
   "source": [
    "# save formatted photos if u don't want to send to openai again\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom JSON encoder to handle datetime objects\n",
    "class DateTimeEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        return super().default(obj)\n",
    "\n",
    "# Save formatted photos to JSON\n",
    "def save_photos_to_json(photos, output_file='formatted_photos_11_26_24_1.json'):\n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(photos, f, cls=DateTimeEncoder, indent=2)\n",
    "        print(f\"Successfully saved photos to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving photos to JSON: {str(e)}\")\n",
    "\n",
    "# Use it\n",
    "save_photos_to_json(formatted_photos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1cbba159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to replace missing photos in the transcript worked to avoid errors but story line made no sense\n",
    "def clean_slideshow_transcript(formatted_photos, transcript_segments):\n",
    "    \"\"\"\n",
    "    Makes sure that the image exists, if not, it replaces the image with the most appropriate photo\n",
    "    \n",
    "    Args:\n",
    "        formatted_photos (list): List of dictionaries containing photo information\n",
    "        transcript_segments (SlideshowTranscript): Transcript with timing and image information\n",
    "        \n",
    "    Returns:\n",
    "        List[SlideSegment]: List of structured segments for the slideshow\n",
    "    \"\"\"\n",
    "    # Find missing paths\n",
    "    missing_paths = find_missing_paths(formatted_photos, transcript_segments)\n",
    "    \n",
    "    if not missing_paths:\n",
    "        return transcript_segments\n",
    "    \n",
    "    client = OpenAI()\n",
    "    \n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"You are doing quality control for a slideshow creation system. \n",
    "                Your job is to fix segments that reference non-existent images by replacing them with the most appropriate \n",
    "                existing image based on the context and descriptions.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"These images don't exist in our source data: {missing_paths}\n",
    "\n",
    "                Please update the transcript segments to use the most appropriate existing image for each missing image.\n",
    "                Consider the context of the segment's text and the descriptions of available images.\n",
    "\n",
    "                Available photos and their descriptions:\n",
    "                {json.dumps(formatted_photos, indent=2)}\n",
    "\n",
    "                Current transcript segments:\n",
    "                {json.dumps(transcript_segments.dict(), indent=2)}\"\"\"\n",
    "            }\n",
    "        ],\n",
    "        response_format=SlideshowTranscript\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddddd0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses moviepy which took way to long and was kinda overkill lol\n",
    "from moviepy import ImageClip, concatenate_videoclips, AudioClip\n",
    "import os\n",
    "\n",
    "def create_slideshow(transcript_segments):\n",
    "    \"\"\"\n",
    "    Create a video slideshow from transcript segments using moviepy\n",
    "    \n",
    "    Args:\n",
    "        transcript_segments (SlideshowTranscript): Transcript with timing and image information\n",
    "    \"\"\"\n",
    "    # Create video clips for each segment\n",
    "    clips = []\n",
    "    \n",
    "    for segment in transcript_segments.segments:\n",
    "        # Create image clip\n",
    "        image_clip = ImageClip(segment.image_path, duration=segment.duration_seconds)\n",
    "\n",
    "        # Center the image\n",
    "        # image_clip = image_clip.set_position('center')\n",
    "        \n",
    "        clips.append(image_clip)\n",
    "    \n",
    "    # Concatenate all clips\n",
    "    final_video = concatenate_videoclips(clips, method=\"compose\")\n",
    "    return final_video\n",
    "\n",
    "    \n",
    "    # Write the video file\n",
    "    # final_video.write_videofile(\n",
    "    #     \"slideshow.mov\",  # Changed extension to .mov\n",
    "    #     fps=10,          # Slideshow so can be slow\n",
    "    #     codec='libx264', \n",
    "    #     preset='medium',\n",
    "    #     audio=False,\n",
    "    # )\n",
    "\n",
    "# Use it\n",
    "result =create_slideshow(transcript_segments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
